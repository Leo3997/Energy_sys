from stable_baselines3 import PPO
from knitting_env import KnittingEnv
import os

# 1. åˆ›å»ºç¯å¢ƒ
env = KnittingEnv()

# 2. å®šä¹‰ PPO æ¨¡å‹
# "MlpPolicy" è¡¨ç¤ºä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœº(ç¥ç»ç½‘ç»œ)æ¥å¤„ç†è¿™ç§æ•°å€¼å‹è¾“å…¥
model = PPO(
    "MlpPolicy",
    env,
    verbose=1,
    learning_rate=0.0003,
    gamma=0.99,
    device='cpu',  # <--- æ–°å¢ï¼šå¼ºåˆ¶ä½¿ç”¨CPUï¼Œæ¶ˆé™¤é‚£ä¸ªé»„è‰²çš„GPUè­¦å‘Šï¼Œå¯¹äºå°æ¨¡å‹CPUåè€Œæ›´å¿«
    tensorboard_log=None  # <--- ä¿®æ”¹ï¼šæš‚æ—¶å…³é—­æ—¥å¿—ï¼Œé¿å¼€è·¯å¾„æŠ¥é”™
)

print("ğŸš€ å¼€å§‹ PPO ç¥ç»ç½‘ç»œè®­ç»ƒ...")
print("AI æ­£åœ¨ç–¯ç‹‚è¯•é”™ï¼šåŠ é€Ÿ -> æ–­çº±(æƒ©ç½š) -> å‡é€Ÿ -> æ•ˆç‡ä½(ä½åˆ†) -> å¯»æ‰¾å¹³è¡¡ç‚¹...")

# 3. å¼€å§‹è®­ç»ƒ
# total_timesteps=50000 æ„å‘³ç€è®© AI ç© 5ä¸‡æ­¥
model.learn(total_timesteps=50000)

# 4. ä¿å­˜è®­ç»ƒå¥½çš„å¤§è„‘
model.save("ppo_knitting_brain")
print("âœ… æ¨¡å‹å·²ä¿å­˜ä¸º ppo_knitting_brain.zip")
